{
 "metadata": {
  "name": "",
  "signature": "sha256:1b53a850596cb223d23f0dd8f23450be04700800221a7da74bbeadc61667d2f5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "An on-line movie recommending service using Spark & Flask - Building the web service  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This tutorial goes into detail into how to use Spark machine learning models, or even other kind of data anlytics objects, within a web service. This open the door to on-line predictions, recommendations, etc. By using the Python language, we make this task very easy, thanks to Spark own Python capabilities, and to Python-based frameworks such as Flask. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This tutorial can be used independently, to build web-services on top of any kind of Spark models. However, it combines powerfully with our tutorial on using Spark MLlib to build a movie recommender model based on the MovieLens dataset. By doing so, you will be able to develop a complete **on-line movie recommendation service**.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our complete web service contains three Python files:  \n",
      "    \n",
      "- `engine.py` defines the recommendation engine, wrapping insde all the Spark related computations.  \n",
      "- `app.py` is a Flask web application that defines a RESTful-like API around the engine.  \n",
      "- `server.py` initialises a *CherryPy* webserver after creating a Spark context and Flask web app using the previous.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But let's explain each of them in detail, together with the pecualiraties of deploying such a system using Spark as a computation engine. We will put the emphasis on how to use a Spark model within the web context we are dealing with. For an explanation on the MovieLens data and how to build the model using Spark, have a look at the tutorial about Building the Model.    "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A recommendation engine"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At the very core of our movie recommendation web service resides a recommendation engine (i.e. `engine.py` in our final deployment). It is represented by the class `RecommendationEngine` and this section will describe step by step how its functionality and implementation.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Starting the engine"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When the engine is initialised, we need to geenrate the ALS model for the first time. Optionally (we won't do it here) we might be loading a previously persisted model in order to use it for recommendations. Moreover, we might need to load or precompute any RDDs that will be used later on to make recommendations.      \n",
      "\n",
      "We will do things of that kind in the `__init__` method of our `RecommendationEngine` class (making use of two private methods). In this case, we won't save any time. We will repeat the whole process every time the engine is creates.    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from pyspark.mllib.recommendation import ALS\n",
      " \n",
      "import logging\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "\n",
      "class RecommendationEngine:\n",
      "    \"\"\"A movie recommendation engine\n",
      "    \"\"\"\n",
      " \n",
      "    def __count_and_average_ratings(self):\n",
      "        \"\"\"Updates the movies ratings counts from \n",
      "        the current data self.ratings_RDD\n",
      "        \"\"\"\n",
      "        logger.info(\"Counting movie ratings...\")\n",
      "        movie_ID_with_ratings_RDD = self.ratings_RDD.map(lambda x: (x[1], x[2])).groupByKey()\n",
      "        movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages)\n",
      "        self.movies_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))\n",
      " \n",
      " \n",
      "    def __train_model(self):\n",
      "        \"\"\"Train the ALS model with the current dataset\n",
      "        \"\"\"\n",
      "        logger.info(\"Training the ALS model...\")\n",
      "        self.model = ALS.train(self.ratings_RDD, self.rank, seed=self.seed,\n",
      "                               iterations=self.iterations, lambda_=self.regularization_parameter)\n",
      "        logger.info(\"ALS model built!\")\n",
      " \n",
      " \n",
      "    def __init__(self, sc, dataset_path):\n",
      "        \"\"\"Init the recommendation engine given a Spark context and a dataset path\n",
      "        \"\"\"\n",
      " \n",
      "        logger.info(\"Starting up the Recommendation Engine: \")\n",
      " \n",
      "        self.sc = sc\n",
      " \n",
      "        # Load ratings data for later use\n",
      "        logger.info(\"Loading Ratings data...\")\n",
      "        ratings_file_path = os.path.join(dataset_path, 'ratings.csv')\n",
      "        ratings_raw_RDD = self.sc.textFile(ratings_file_path)\n",
      "        ratings_raw_data_header = ratings_raw_RDD.take(1)[0]\n",
      "        self.ratings_RDD = ratings_raw_RDD.filter(lambda line: line!=ratings_raw_data_header)\\\n",
      "            .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()\n",
      "        # Load movies data for later use\n",
      "        logger.info(\"Loading Movies data...\")\n",
      "        movies_file_path = os.path.join(dataset_path, 'movies.csv')\n",
      "        movies_raw_RDD = self.sc.textFile(movies_file_path)\n",
      "        movies_raw_data_header = movies_raw_RDD.take(1)[0]\n",
      "        self.movies_RDD = movies_raw_RDD.filter(lambda line: line!=movies_raw_data_header)\\\n",
      "            .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2])).cache()\n",
      "        self.movies_titles_RDD = self.movies_RDD.map(lambda x: (int(x[0]),x[1])).cache()\n",
      "        # Pre-calculate movies ratings counts\n",
      "        self.__count_and_average_ratings()\n",
      " \n",
      "        # Train the model\n",
      "        self.rank = 8\n",
      "        self.seed = 5L\n",
      "        self.iterations = 10\n",
      "        self.regularization_parameter = 0.1\n",
      "        self.__train_model() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All the code form the `__init__` and the two private methods has been explained in the tutorial about Building the Model.   "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Adding new ratings"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When using *Collaborative Filtering* and Spark's *Alternating Least Squares*, we need to recompute the prediction model for every new batch of user ratings. This was explained in our previous tutorial on building the model.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_ratings(self, ratings):\n",
      "    \"\"\"Add additional movie ratings in the format (user_id, movie_id, rating)\n",
      "    \"\"\"\n",
      "    # Convert ratings to an RDD\n",
      "    new_ratings_RDD = self.sc.parallelize(ratings)\n",
      "    # Add new ratings to the existing ones\n",
      "    self.ratings_RDD = self.ratings_RDD.union(new_ratings_RDD)\n",
      "    # Re-compute movie ratings count\n",
      "    self.__count_and_average_ratings()\n",
      "    # Re-train the ALS model with the new ratings\n",
      "    self.__train_model()\n",
      "\n",
      "    return ratings\n",
      "\n",
      "# Attach the function to a class method\n",
      "RecommendationEngine.add_ratings = add_ratings"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Making recommendations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also explained how to make recommendations with our ALS model in the tutorial about building the movie recommender. Here, we will basically repeat equivalent code, wrapped inside a method of our `RecommendationEnginer` class, and making use of a private method that will be used for every predicting method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def __predict_ratings(self, user_and_movie_RDD):\n",
      "    \"\"\"Gets predictions for a given (userID, movieID) formatted RDD\n",
      "    Returns: an RDD with format (movieTitle, movieRating, numRatings)\n",
      "    \"\"\"\n",
      "    predicted_RDD = self.model.predictAll(user_and_movie_RDD)\n",
      "    predicted_rating_RDD = predicted_RDD.map(lambda x: (x.product, x.rating))\n",
      "    predicted_rating_title_and_count_RDD = \\\n",
      "        predicted_rating_RDD.join(self.movies_titles_RDD).join(self.movies_rating_counts_RDD)\n",
      "    predicted_rating_title_and_count_RDD = \\\n",
      "        predicted_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))\n",
      "\n",
      "    return predicted_rating_title_and_count_RDD\n",
      "    \n",
      "def get_top_ratings(self, user_id, movies_count):\n",
      "    \"\"\"Recommends up to movies_count top unrated movies to user_id\n",
      "    \"\"\"\n",
      "    # Get pairs of (userID, movieID) for user_id unrated movies\n",
      "    user_unrated_movies_RDD = self.ratings_RDD.filter(lambda rating: not rating[1]==user_id).map(lambda x: (user_id, x[1]))\n",
      "    # Get predicted ratings\n",
      "    ratings = self.__predict_ratings(user_unrated_movies_RDD).filter(lambda r: r[2]>=25).takeOrdered(movies_count, key=lambda x: -x[1])\n",
      "\n",
      "    return ratings\n",
      "\n",
      "# Attach the functions to class methods\n",
      "RecommendationEngine.__predict_ratings = __predict_ratings\n",
      "RecommendationEngine.get_top_ratings = get_top_ratings"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apart form getting the top unrated movies, we will also want to get ratings to particular movies. We will do so with a new mothod in our `RecommendationEngine`.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_ratings_for_movie_ids(self, user_id, movie_ids):\n",
      "    \"\"\"Given a user_id and a list of movie_ids, predict ratings for them \n",
      "    \"\"\"\n",
      "    requested_movies_RDD = self.sc.parallelize(movie_ids).map(lambda x: (user_id, x))\n",
      "    # Get predicted ratings\n",
      "    ratings = self.__predict_ratings(requested_movies_RDD).collect()\n",
      "\n",
      "    return ratings\n",
      "\n",
      "# Attach the function to a class method\n",
      "RecommendationEngine.get_ratings_for_movie_ids = get_ratings_for_movie_ids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building a web API around our engine using Flask"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Flask](http://flask.pocoo.org/) is a web microframework for Python. It is very easy to start up a web API, by just importing in in our script and using some annotations to associate our service end-points with Python functions. In our case we will wrap our `RecommendationEngine` methods around some of these end-points and interchange `JSON` formatted data with the web client.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In fact is so simple that we will show the whole `app.py` here, instead of going piece by piece."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from flask import Blueprint\n",
      "main = Blueprint('main', __name__)\n",
      " \n",
      "import json\n",
      "from engine import RecommendationEngine\n",
      " \n",
      "import logging\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "logger = logging.getLogger(__name__)\n",
      " \n",
      "from flask import Flask, request\n",
      " \n",
      "@main.route(\"/<int:user_id>/ratings/top/<int:count>\", methods=[\"GET\"])\n",
      "def top_ratings(user_id, count):\n",
      "    logger.debug(\"User %s TOP ratings requested\", user_id)\n",
      "    top_ratings = recommendation_engine.get_top_ratings(user_id,count)\n",
      "    return json.dumps(top_ratings)\n",
      " \n",
      "@main.route(\"/<int:user_id>/ratings/<int:movie_id>\", methods=[\"GET\"])\n",
      "def movie_ratings(user_id, movie_id):\n",
      "    logger.debug(\"User %s rating requested for movie %s\", user_id, movie_id)\n",
      "    ratings = recommendation_engine.get_ratings_for_movie_ids(user_id, [movie_id])\n",
      "    return json.dumps(ratings)\n",
      " \n",
      " \n",
      "@main.route(\"/<int:user_id>/ratings\", methods = [\"POST\"])\n",
      "def add_ratings(user_id):\n",
      "    # get the ratings from the Flask POST request object\n",
      "    ratings_list = request.form.keys()[0].strip().split(\"\\n\")\n",
      "    ratings_list = map(lambda x: x.split(\",\"), ratings_list)\n",
      "    # create a list with the format required by the negine (user_id, movie_id, rating)\n",
      "    ratings = map(lambda x: (user_id, int(x[0]), float(x[1])), ratings_list)\n",
      "    # add them to the model using then engine API\n",
      "    recommendation_engine.add_ratings(ratings)\n",
      " \n",
      "    return json.dumps(ratings)\n",
      " \n",
      " \n",
      "def create_app(spark_context, dataset_path):\n",
      "    global recommendation_engine \n",
      " \n",
      "    recommendation_engine = RecommendationEngine(spark_context, dataset_path)    \n",
      "    \n",
      "    app = Flask(__name__)\n",
      "    app.register_blueprint(main)\n",
      "    return app"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Basically we use the app as follows:  \n",
      "\n",
      "- We init the thing when calling `create_app`. Here the `RecommendationEngine` object is created and then we associate the `@main.route` annotations defined above. Each annotation is defined by (see [Flask docs](http://flask.pocoo.org/docs/0.10/)):  \n",
      " - A route, that is its URL and may contain parameters between <>. They are mapped to the function arguments.  \n",
      " - A list of HTTP available methods.  \n",
      "- There are three of these annotations defined, that correspond with the three `RecommendationEngine` methods:  \n",
      "  - `GET /<user_id>/ratings/top` get top recommendations from the engine.  \n",
      "  - `GET /<user_id>/ratings` get predicted rating for a individual movie.  \n",
      "  - `POST /<user_id>/ratings` add new ratings. The format is a series of lines (ending with the newline separator) with `movie_id` and `rating` separated by commas. For example, the following file corresponds to the ten new user ratings used as a example in the tutorial about building the model:    \n",
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`260,9  \n",
      "1,8  \n",
      "16,7  \n",
      "25,8  \n",
      "32,9  \n",
      "335,4  \n",
      "379,3  \n",
      "296,7  \n",
      "858,10  \n",
      "50,8`  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Deploying a WSGI server using CherryPy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Among other things, the [CherryPy framework](http://www.cherrypy.org/) features a reliable, HTTP/1.1-compliant, WSGI thread-pooled webserver. It is also easy to run multiple HTTP servers (e.g. on multiple ports) at once. All this makes it a perfect candidate for an easy to deploy production web server for our on-line recommendation service.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The use that we will make of the CherryPy server is relatively simple. Again we will show here the complete `server.py` script and then explain it a bit.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time, sys, cherrypy, os\n",
      "from paste.translogger import TransLogger\n",
      "from app import create_app\n",
      "from pyspark import SparkContext, SparkConf\n",
      " \n",
      "def init_spark_context():\n",
      "    # load spark context\n",
      "    conf = SparkConf().setAppName(\"movie_recommendation-server\")\n",
      "    # IMPORTANT: pass aditional Python modules to each worker\n",
      "    sc = SparkContext(conf=conf, pyFiles=['engine.py', 'app.py'])\n",
      " \n",
      "    return sc\n",
      " \n",
      " \n",
      "def run_server(app):\n",
      " \n",
      "    # Enable WSGI access logging via Paste\n",
      "    app_logged = TransLogger(app)\n",
      " \n",
      "    # Mount the WSGI callable object (app) on the root directory\n",
      "    cherrypy.tree.graft(app_logged, '/')\n",
      " \n",
      "    # Set the configuration of the web server\n",
      "    cherrypy.config.update({\n",
      "        'engine.autoreload.on': True,\n",
      "        'log.screen': True,\n",
      "        'server.socket_port': 5432,\n",
      "        'server.socket_host': '0.0.0.0'\n",
      "    })\n",
      " \n",
      "    # Start the CherryPy WSGI web server\n",
      "    cherrypy.engine.start()\n",
      "    cherrypy.engine.block()\n",
      " \n",
      " \n",
      "if __name__ == \"__main__\":\n",
      "    # Init spark context and load libraries\n",
      "    sc = init_spark_context()\n",
      "    dataset_path = os.path.join('datasets', 'ml-latest')\n",
      "    app = create_app(sc, dataset_path)\n",
      " \n",
      "    # start web server\n",
      "    run_server(app)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is pretty standard use of `CherryPy`. If we have a look at the `__main__` entry point, we do three things:  \n",
      "\n",
      "- Create a spark context as defined in the function `init_spark_context`, passing aditional Python modules there.  \n",
      "- Create the Flask app calling the `create_app` we defined in `app.py`.  \n",
      "- Run the server itself.  \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "See the following section about starting the server.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Running the server with Spark"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to have the server running while being able to access a Spark context and cluster, we need to submit the `server.py` file to `pySpark` by using `spark-submit`. The different parameters when using this command are better explained in the [Spark](https://spark.apache.org/docs/latest/submitting-applications.html) docummentaion. In our case, we will use something like the following."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "~/spark-1.3.1-bin-hadoop2.6/bin/spark-submit --master spark://169.254.206.2:7077 --total-executor-cores 14 --executor-memory 6g server.py "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The important bits are:  \n",
      "\n",
      "- Use `spark-submit` and not `pyspark` directly.  \n",
      "- The `--master` parameters must point to your Spark cluster setup (can be local).  \n",
      "- You can pass additional configuration parameters such as `--total-executor-cores` and `--executor-memory`  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You will see an output like the following:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "INFO:engine:Starting up the Recommendation Engine: \n",
      "INFO:engine:Loading Ratings data...\n",
      "INFO:engine:Loading Movies data...\n",
      "INFO:engine:Counting movie ratings...\n",
      "INFO:engine:Training the ALS model...\n",
      "       ... More Spark and CherryPy logging\n",
      "INFO:engine:ALS model built!                                                                                                 \n",
      "[05/Jul/2015:14:06:29] ENGINE Bus STARTING\n",
      "[05/Jul/2015:14:06:29] ENGINE Started monitor thread 'Autoreloader'.\n",
      "[05/Jul/2015:14:06:29] ENGINE Started monitor thread '_TimeoutMonitor'.\n",
      "[05/Jul/2015:14:06:29] ENGINE Serving on http://0.0.0.0:5432\n",
      "[05/Jul/2015:14:06:29] ENGINE Bus STARTED"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Some considerations when using multiple Scripts and Spark-submit"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two issues we need to work around when using Spark in a deployment like this. The first one is that a Spark cluster is a distrubuted environment of **Workers** orchestrated from the Spark **Master** where the Python script is launched. This means that the master is the only one with access to the submitted script and local additional files. If we want the workers to be able to access additional imported Python moules, they either have to be part of our Python distributuon or we need to pass them implicitly. We do this by using the `pyFiles=['engine.py', 'app.py']` parameter when creating the `SparkContext` object. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The second issue is related with the previous one but is a bit more tricky. In Spark, when using transformations (e.g. `map` on an RDD), we cannot make reference to other RDDs or objects that are not globally available in the execution context. For example, we cannot make reference to a class instance variables. Because of this, we have defined all the functions that are passed to RDD transformations outside the `RecommendationEgine` class.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Trying the service"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's now give the service a try, using the same data we used on the tutorial about building the model. That is, first we are going to add ratings, and then we are going to get top ratings and individual ratings."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "POSTing new ratings"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So first things first, we need to have our service runing as explained in the previous section. Once is running, we will use `curl` to post new ratings from the shell. If we have the file `user_ratings.file` (see **Getting the source code** below) in the current folder, just execute the following command.  "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "curl --data-binary @user_ratings.file http://<SERVER_IP>:5432/0/ratings"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Replacing `<SERVER_IP>` with the IP address where you have the server running (e.g. `localhost` or `127.0.0.1` if its running locally). This command will start some computations and end up with an output representing the ratings that has been submitted as a list of lists (this is just to check that the process was successfull)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the server output window you will see the actual Spark computation output together with CherryPy's output messages about HTTP requests.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "GETing top recommendations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This one you can do it using also curl, but the JSON rendering will be better if you just use your web browser. Just go to `http://<SERVER_IP>:5432/0/ratings/top/10` to get the top 10 movie recommendations for the user we POSTed recommendations for.  The results should match with those seen in the tutorial about Building the Model.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "GETing individual ratings"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Similarly, we can curl/navigate to `http://<SERVER_IP>:5432/0/ratings/500 ` to get the predicted rating for the movie *The Quiz (1994)*."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting the source code"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The source code for the three Python files, together with additional files, that compose our web service can be found in the [following GISTs](https://gist.github.com/jadianes/85c31c72dc96b036372e)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
